{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importing Necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amith\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_graph(seed=42):\n",
    "    '''Function to reset the tensorflow graph to default'''\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     6,
     52
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose a training size in decimal 0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amith\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing a Recurrent neural network with 3 cells and 100 neurons in each cell \n",
      "\n",
      "Please choose a drop out rate \n",
      "0.15\n",
      "Choose no. of epochs 800\n",
      "Choose batch size 20\n",
      "Training a Recurrent neural network for 800 epochs  \n",
      "\n",
      "0 Train accuracy:  0.95\n",
      "100 Train accuracy:  1.0\n",
      "200 Train accuracy:  1.0\n",
      "300 Train accuracy:  1.0\n",
      "400 Train accuracy:  1.0\n",
      "500 Train accuracy:  1.0\n",
      "600 Train accuracy:  1.0\n",
      "700 Train accuracy:  1.0\n",
      "INFO:tensorflow:Restoring parameters from ./cancer_tf_test.ckpt\n",
      "Accuracy of the predictions \n",
      " 0.9649122807017544 \n",
      "\n",
      "First 10 Predictions  [1 0 0 1 0 0 0 0 0 1] \n",
      " First 10 Actual values  [1 0 0 1 0 0 0 0 0 1] \n",
      "\n",
      "Classification report \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.99      0.97        75\n",
      "          1       0.97      0.92      0.95        39\n",
      "\n",
      "avg / total       0.97      0.96      0.96       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Multi_RNN():\n",
    "    \n",
    "    '''Class that constructs Multiple Recurrent Neural Network'''\n",
    "    \n",
    "    strt_ind = 0; end_ind = 0 #Index values for batch processing\n",
    "    \n",
    "    def __init__(self, n_steps, n_neurons, n_inputs, n_out, lr, n_layers):\n",
    "        \n",
    "        #Setting the hyper parameters from the user\n",
    "        self.n_steps = n_steps\n",
    "        self.n_neurons = n_neurons\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_out = n_out\n",
    "        self.lr = lr\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        #Initializing default train and test variables\n",
    "        self.X_train, self.X_test, self.y_train,self.y_test = None,None,None,None\n",
    "        \n",
    "        \n",
    "        #Reading the data into a data frame\n",
    "        self.df = pd.read_csv('C:\\\\Users\\\\amith\\\\Downloads\\\\data.csv')\n",
    "        \n",
    "        \n",
    "        self.df = self.df.dropna(axis=1)\n",
    "        \n",
    "        \n",
    "        #Encoding the target columns which is in string format to a binary format\n",
    "        self.df['diagnosis'] = self.df['diagnosis'].astype('category')\n",
    "        self.df['diag_cat'] = self.df['diagnosis'].cat.codes\n",
    "        \n",
    "        \n",
    "        #Initializing standard scaler to normalize the input data\n",
    "        self.s = StandardScaler()        \n",
    "        \n",
    "        \n",
    "        #Fit and transform the input data by normal scaling\n",
    "        x_vals = self.s.fit_transform(self.df.drop(['id', 'diagnosis', 'diag_cat'], axis=1))\n",
    "        \n",
    "        \n",
    "        #Create a new data frame for independent variables\n",
    "        X_Vals = pd.DataFrame(x_vals, columns=[ self.df.columns[2:32] ])\n",
    "        y_Vals = self.df['diag_cat']\n",
    "        \n",
    "        \n",
    "        #Choose training size to split the data\n",
    "        self.training_size = float(input('Choose a training size in decimal '))\n",
    "                \n",
    "        \n",
    "        #Split the data in to train and test variables\n",
    "        self.X_train,self.X_test, self.y_train,self.y_test = train_test_split(X_Vals, y_Vals, train_size = self.training_size)\n",
    "     \n",
    "    def next_batch(self,iteration,batch_size):\n",
    "        \n",
    "        '''Returns batch of feature and target variables based on index values :\n",
    "        \n",
    "        inputs:\n",
    "        -> Iteration: index of i-th iteration of training\n",
    "        \n",
    "        -> batch_size: the batch size specified\n",
    "        \n",
    "        -> return type: data frame'''\n",
    "        \n",
    "        global strt_ind, end_ind\n",
    "         \n",
    "        \n",
    "        if iteration==0:\n",
    "            \n",
    "            strt_pos = 0 \n",
    "            end_pos = batch_size\n",
    "            \n",
    "            \n",
    "            X_batch = self.X_train[strt_pos:end_pos]\n",
    "            y_batch = self.y_train[strt_pos:end_pos]\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            strt_pos = iteration * batch_size\n",
    "            end_pos = strt_pos + batch_size\n",
    "            \n",
    "            \n",
    "            \n",
    "            X_batch = self.X_train[strt_pos:end_pos]\n",
    "            y_batch = self.y_train[strt_pos:end_pos]\n",
    "    \n",
    "    \n",
    "        \n",
    "        return X_batch, y_batch\n",
    "    \n",
    "    def Construct_RNN(self):\n",
    "        \n",
    "        '''This function constructs a Recurrent neural network  with the hyper parameters provided and internally calls \n",
    "        the train method to train the data.\n",
    "        \n",
    "        return : None'''\n",
    "        \n",
    "        #Reset the graph if already exists\n",
    "        reset_graph()\n",
    "        \n",
    "        print('Constructing a Recurrent neural network with {} cells and {} neurons in each cell'.format(self.n_layers,self.n_neurons), '\\n')\n",
    "        \n",
    "        # Place holders for X and y\n",
    "        self.X = tf.placeholder(tf.float32, shape=[None, self.n_steps, self.n_inputs])\n",
    "        \n",
    "        self.y = tf.placeholder(tf.int32, shape=[None])\n",
    "        \n",
    "        \n",
    "        #Set a training layer\n",
    "        self.training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "        self.dropout_rate = float(input('Please choose a drop out rate \\n'))\n",
    "        \n",
    "        with tf.name_scope(\"RecNN\"): \n",
    "            \n",
    "            #Probability of dropping a neuron in each cell\n",
    "            self.X_drop = tf.layers.dropout(self.X, self.dropout_rate,training=self.training)\n",
    "\n",
    "            #Creating multiple Recurrent Neural Network cells\n",
    "            cells = [tf.contrib.rnn.BasicRNNCell(num_units=self.n_neurons, activation=tf.nn.tanh)\n",
    "                    for layer in range(self.n_layers)]\n",
    "\n",
    "            #Construction of a Multiple Recurrent Neural Network Cell\n",
    "            multi_layer_cell = tf.contrib.rnn.MultiRNNCell(cells,state_is_tuple=False)\n",
    "\n",
    "            #Copy the output and states of an unrolled cell\n",
    "            self.outputs,self.states = tf.nn.dynamic_rnn(multi_layer_cell,self.X_drop, dtype=tf.float32)\n",
    "\n",
    "            \n",
    "            #Heinitializer\n",
    "            he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "            \n",
    "            #Create a densely connected recurrent neural network layer\n",
    "            self.logits = tf.layers.dense(self.states,self.n_out, kernel_initializer=he_init)\n",
    "            \n",
    "        with tf.name_scope(\"loss\"):    \n",
    "            xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = self.y, logits = self.logits)\n",
    "\n",
    "            #Specify the loss function\n",
    "            self.loss = tf.reduce_mean(xentropy)\n",
    "        \n",
    "        with tf.name_scope(\"train\"):\n",
    "            \n",
    "            #Choosing an Adam Optimizer\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=self.lr)\n",
    "            \n",
    "            #Minimize the training loss\n",
    "            self.train_op = optimizer.minimize(self.loss)\n",
    "        \n",
    "        with tf.name_scope(\"eval\"):\n",
    "            \n",
    "            #Choosing the top most prediction from the probability of values\n",
    "            correct = tf.nn.in_top_k(self.logits, self.y,1)\n",
    "            \n",
    "            self.acc = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "        \n",
    "        #Initializing the global variables\n",
    "        self.init = tf.global_variables_initializer()\n",
    "        \n",
    "        #Save the model to disk\n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "        #Train the constructed model\n",
    "        self.train_it()\n",
    "    \n",
    "    def train_it(self):\n",
    "        \n",
    "        '''This function trains the model which was constructed before and internally calls the test method\n",
    "        \n",
    "        return type: None'''\n",
    "        \n",
    "        # Taking the no.of rows and columns of the data frame\n",
    "        r,c = self.df.shape\n",
    "        \n",
    "        \n",
    "        ep = int(input('Choose no. of epochs '))\n",
    "        b_s = int(input('Choose batch size '))\n",
    "        \n",
    "        print('Training a Recurrent neural network for {} epochs '.format(ep), '\\n')\n",
    "        \n",
    "        #Calculating the total training size\n",
    "        total_training_size = int(r * self.training_size)\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            self.init.run()\n",
    "            \n",
    "            for epoch in range(ep):\n",
    "                \n",
    "                for it in range(total_training_size // b_s):\n",
    "                    \n",
    "                    #Fetching next batch of inputs\n",
    "                    X_b, y_b = self.next_batch(it,b_s)\n",
    "                    \n",
    "                    #Reshaping the batch of inputs to a 3D tensor of shape [batch_size, no. of steps, no. of inputs]\n",
    "                    X_batch = np.array(X_b).reshape((-1, self.n_steps,self.n_inputs))\n",
    "                    \n",
    "                    \n",
    "                    #Running the training session\n",
    "                    sess.run([self.train_op, self.loss], \n",
    "                                        feed_dict={self.training: True,self.X: X_batch, self.y: y_b})\n",
    "                    \n",
    "                #Predicting the accuracy of the training session\n",
    "                acc_t = self.acc.eval(feed_dict={self.X: X_batch, self.y: y_b})\n",
    "                \n",
    "                \n",
    "                if epoch % 100 == 0:\n",
    "                    print(epoch, 'Train accuracy: ', acc_t)\n",
    "                    \n",
    "                          \n",
    "    \n",
    "            #Saving the model \n",
    "            save_path = self.saver.save(sess, \"./cancer_tf_test.ckpt\")\n",
    "        \n",
    "        #Testing the model\n",
    "        self.test_it()\n",
    "        \n",
    "    def test_it(self):\n",
    "        \n",
    "        '''Tests the model on testing set'''\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            \n",
    "            #invoking the saved model\n",
    "            self.saver.restore(sess, \"./cancer_tf_test.ckpt\")\n",
    "            \n",
    "            #Testing the model on test set \n",
    "            Z = self.logits.eval(feed_dict={self.X: np.array(self.X_test).reshape((-1,self.n_steps,self.n_inputs))  })\n",
    "            \n",
    "            #Fetching the top predictions from evaluations\n",
    "            y_pred = np.argmax(Z, axis=1)\n",
    "            \n",
    "            \n",
    "            #Various accuracy metrics\n",
    "            print('Accuracy of the predictions \\n', accuracy_score(y_true= self.y_test, y_pred=y_pred), '\\n')\n",
    "            \n",
    "            print('First 10 Predictions ', y_pred[0:10], '\\n', 'First 10 Actual values ', np.array(self.y_test[0:10]), '\\n')\n",
    "            \n",
    "            print('Classification report \\n' , classification_report(y_true= self.y_test, y_pred= y_pred))   \n",
    "            \n",
    "    def main(self):\n",
    "        \n",
    "        '''Main Function'''\n",
    "        \n",
    "        self.Construct_RNN()\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    Multi_RNN(30,100,1,2,0.001, 3).main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Precision: Of all the values which were predicted as 0/Benign; 96% of them were correctly predicted as Benign's and 4% of them were wrongly predicted as Benign's. Similarly, for 1's or Malign's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Recall: Of the all the values which were actually 0/Benign; the model was able to CATCH 99% of them correctly. It wasn't able to properly identify 1% of values as Benign. Similarly for 1/Malign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
